<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames.">
    <meta name="keywords" content="6D Object Pose, Multi-view Optimization, Active Vision, Deep Learning">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Active 6D Pose Estimation for Textureless Objects using
                        Multi-View RGB Frames</h1>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
              Jun Yang<sup>1, 2</sup>,</span>
                        <span class="author-block">
              Wenjie Xue<sup>2</sup>,</span>
                        <span class="author-block">
              Sahar Ghavidel<sup>2</sup>,
            </span>
                        <span class="author-block">
              Steven L. Waslander<sup>1</sup>,
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>University of Toronto Institute for Aerospace Studies and Robotics Institute,</span>
                        <span class="author-block"><sup>2</sup>Epson Canada Ltd.</span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.03726"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!-- Video Link. -->
                            <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
                            <!--              &lt;!&ndash; Code Link. &ndash;&gt;-->
                            <!--              <span class="link-block">-->
                            <!--                <a href="https://github.com/TRAILab/ActiveODPE"-->
                            <!--                   class="external-link button is-normal is-rounded is-dark">-->
                            <!--                  <span class="icon">-->
                            <!--                      <i class="fab fa-github"></i>-->
                            <!--                  </span>-->
                            <!--                  <span>Code</span>-->
                            <!--                  </a>-->
                            <!--              </span>-->
                            <!-- Dataset Link. -->
                            <span class="link-block">
                <a href="https://www.trailab.utias.utoronto.ca/t-robi-dataset"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
                            <!-- Evaluation Link. -->
                            <span class="link-block">
                <a href="https://github.com/TRAILab/ActiveODPE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-chart-bar"></i>
                  </span>
                  <span>Evaluation</span>
                </a>
              </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <!--    <div class="hero-body">-->
        <!--      <video id="teaser" autoplay muted loop playsinline height="100%">-->
        <!--        <source src="./static/videos/teaser.mp4"-->
        <!--                type="video/mp4">-->
        <!--      </video>-->
        <!--      <h2 class="subtitle has-text-centered">-->
        <!--        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into-->
        <!--        free-viewpoint-->
        <!--        portraits.-->
        <!--      </h2>-->
        <!--    </div>-->
        <div class="hero-body">
            <img src="./static/images/pipeline.png" alt="Pipeline overview"/>
            <p><strong> In this work, we present a complete framework of multi-view
                pose estimation and next-best-view prediction for textureless
                objects.</strong></p>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Estimating the 6D pose of textureless objects from RBG images is an important problem in
                        robotics. Due to appearance ambiguities, rotational symmetries, and severe occlusions,
                        single-view based 6D pose estimators are still unable to handle a wide range of objects,
                        motivating research towards multi-view pose estimation and next-best-view prediction that
                        addresses these limitations.
                    </p>
                    <p>
                        In this work, we propose a comprehensive active perception framework for estimating the 6D poses
                        of textureless objects using only RGB images. Our approach is built upon a key idea: decoupling
                        the 6D pose estimation into a two-step sequential process can greatly improve both accuracy and
                        efficiency. First, we estimate the 3D translation of each object, resolving scale and depth
                        ambiguities inherent to RGB images. These estimates are then used to simplify the subsequent
                        task of determining the 3D orientation, which we achieve through canonical scale template
                        matching. Building on this formulation, we then introduce an active perception strategy that
                        predicts the next best camera viewpoint to capture an RGB image, effectively reducing object
                        pose uncertainty and enhancing pose accuracy.
                    </p>
                    <p>
                        We evaluate our method on the public ROBI and TOD datasets. Additionally, we reconstructed a
                        challenging transparent object dataset and created a large-scale synthetic dataset,
                        corresponding to ROBI and our transparent dataset, which is used to train the network. Using the
                        same camera viewpoints, our multi-view pose estimation significantly outperforms
                        state-of-the-art approaches. Moreover, by leveraging our next-best-view strategy, our approach
                        achieves high pose accuracy with fewer viewpoints than heuristic-based policies.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!--/ Abstract. -->


<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="color: #85B737;">Qualitative Results</h2>
    <p><strong>
      <!-- Given single-frame multi-view cameras as input and without test-time per-scene optimization, DistillNeRF can reconstruct RGB images (row 2), estimate depth (row 3), render foundation model features (rows 4, 5) which enables open-vocabulary text queries (rows 6, 7, 8), and predict binary and semantic occupancy in zero shot (rows 9, 10). -->
          </strong></p>
    <div class="hero-body">
      <img src="./static/images/eval_result.png" alt="Result visualization">

    </div>
  </div>
</section>


<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{yang2025active6dposeestimation,
      title={Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames},
      author={Jun Yang and Wenjie Xue and Sahar Ghavidel and Steven L. Waslander},
      year={2025},
}</code></pre>
    </div>
</section>


<footer class="footer">
    <div class="container">
        <div class="content has-text-centered">
            <!-- <a class="icon-link" href="">
              <i class="fas fa-file-pdf"></i>
            </a> -->
            <!-- <a class="icon-link" href="" class="external-link" disabled>
              <i class="fab fa-github"></i> -->
            <!--      </a>-->
        </div>
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website is licensed under a <a rel="license"
                                                            href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                    <p>
                        We thank the authors of <a href="https://nerfies.github.io/">Nerfies</a> that kindly open
                        sourced the
                        template of this website.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>